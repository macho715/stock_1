# Enhanced subset-matching with HVDC combined-code expansion per user's rule.
# Enhanced with ONTOLOGY-based HVDC CODE structure validation and normalization.
# 
# HVDC CODE Structure (from Ontology System):
#  - Format: HVDC-ADOPT-HE-0001 í˜•íƒœì˜ ê³ ìœ  ì‹ë³„ì
#  - Part 1: HVDC (í”„ë¡œì íŠ¸ ì‹ë³„ì)
#  - Part 2: ADOPT (í”„ë¡œì íŠ¸ íƒ€ì…)  
#  - Part 3: HE/SIM/SCT ë“± (ë²¤ë” ì½”ë“œ)
#  - Part 4: 0001 (ìˆ«ì ID, 4ìë¦¬ íŒ¨ë”©)
#  - Part 5: ì„œë¸Œ ì‹ë³„ì (ì˜µì…˜, ì˜ˆ: -1, -2)
#
# Examples to support:
#  - "HVDC-ADOPT-HE-0325-1, 0325-2"  â†’ {"HVDC-ADOPT-HE-0325-1", "HVDC-ADOPT-HE-0325-2"}
#  - "HVDC-ADOPT-HE-0087,90"         â†’ {"HVDC-ADOPT-HE-0087", "HVDC-ADOPT-HE-0090"}
#  - "HVDC-ADOPT-HE-0192,195,189,193,197,191" â†’ expand each to 4-digit numbers with same prefix
#
# We then form the candidate pool as the union of ALL rows whose FULL code matches any expanded code
# OR whose parts(1..4) match the invoice parts (for each expanded code). Vendor filtering enhanced with ontology rules.
# Subset matching (k packages) is done on this pooled candidate set. Tolerance Â±0.10.

import pandas as pd
import numpy as np
from itertools import combinations
from pathlib import Path
import re
from difflib import SequenceMatcher

# ğŸ¯ REASON ì½”ë“œ ì •ì˜
REASON = {
    "RATE_DIFF": "Invoice rate â‰  Contract rate",
    "PASSTHROUGH_MISMATCH": "Passthrough amount â‰  Invoice total",
    "NOCHARGE_VIOLATION": "No-charge policy (MOSB) but invoice amount > 0",
    "PRORATION_MISMATCH": "Movement/proration mismatch (AvgSQM vs billed SQM)",
    "NAME_MISMATCH": "Warehouse name normalization mismatch",
    "MODE_MISSING": "Billing mode missing",
    "RATE_MISSING": "Contract rate missing",
}

INVOICE_PATH = "HVDC WH IVOICE_0921.xlsx"  # ì‚¬ìš©ì ì§€ì • íŒŒì¼ ì‚¬ìš©
ALL_PATH     = "hvdc.xlsx"  # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ hvdc.xlsx íŒŒì¼ ì‚¬ìš©
OUT_PATH     = Path("HVDC_Invoice_Validation_Dashboard.xlsx")  # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
TOL          = 0.10
MAX_EXACT_N  = 18
USE_MONTH_FILTER = False
ALL_DATE_COL     = "ì…ê³ ì¼ì"
INV_DATE_COL     = "Operation Date"
# Enhanced Vendor Classification (from Ontology System)
VENDOR_ALLOWED = {"HE", "SIM"}  # Primary vendors
VENDOR_EXTENDED = {"HE", "SIM", "SCT", "SEI", "PPL", "MOSB", "ALM", "SHU", "NIE", "ALS", "SKM", "SAS"}  # Extended vendor list

# Warehouse Classification (from Ontology)
WAREHOUSE_INDOOR = {"DSV Indoor", "DSV Al Markaz", "Hauler Indoor"}
WAREHOUSE_OUTDOOR = {"DSV Outdoor", "DSV MZP", "MOSB"}
WAREHOUSE_SITE = {"AGI", "DAS", "MIR", "SHU"}
WAREHOUSE_DANGEROUS = {"AAA Storage", "Dangerous Storage"}

# âœ… NEW: ê³¼ê¸ˆ ëª¨ë“œ ë¶„ë¥˜ (v3.0-correctedì™€ ì¼ê´€ì„± ìœ ì§€)
BILLING_MODE_RATE = {"DSV Outdoor", "DSV MZP", "DSV Indoor", "DSV Al Markaz"}
BILLING_MODE_PASSTHROUGH = {"AAA Storage", "Hauler Indoor", "DHL Warehouse"}
BILLING_MODE_NO_CHARGE = {"MOSB"}

# âœ… NEW: ê³„ì•½ ë‹¨ê°€ (AED/sqm/month) - Rate ëª¨ë“œë§Œ ì ìš©
WAREHOUSE_RATES = {
    'DSV Outdoor': 18.0,
    'DSV MZP': 33.0,
    'DSV Indoor': 47.0,
    'DSV Al Markaz': 47.0,
    'AAA Storage': 0.0,     # Passthrough
    'Hauler Indoor': 0.0,   # Passthrough  
    'DHL Warehouse': 0.0,   # Passthrough
    'MOSB': 0.0,           # No-charge
}

# âœ… NEW: ì°½ê³ ëª… ì •ê·œí™” ë§¤í•‘ (ìŠ¤í ë§/ê³µë°± ì¼ì¹˜ ê°•ì œ)
WAREHOUSE_NAME_MAPPING = {
    # í‘œì¤€ëª… â†’ ë³€í˜•ëª…ë“¤
    'DSV Al Markaz': ['DSV Al Markaz', 'DSV AlMarkaz', 'Al Markaz', 'AlMarkaz'],
    'DSV Indoor': ['DSV Indoor', 'DSVIndoor', 'Indoor'],
    'DSV Outdoor': ['DSV Outdoor', 'DSVOutdoor', 'Outdoor'],
    'DSV MZP': ['DSV MZP', 'DSVMZP', 'MZP'],
    'AAA Storage': ['AAA Storage', 'AAAStorage', 'AAA'],
    'Hauler Indoor': ['Hauler Indoor', 'HaulerIndoor', 'Hauler'],
    'DHL Warehouse': ['DHL Warehouse', 'DHLWarehouse', 'DHL'],
    'MOSB': ['MOSB', 'MOSB Storage']
}

def normalize_warehouse_name(warehouse_name: str) -> str:
    """
    ì°½ê³ ëª…ì„ í‘œì¤€ëª…ìœ¼ë¡œ ì •ê·œí™”
    
    Args:
        warehouse_name: ì›ë³¸ ì°½ê³ ëª…
    Returns:
        str: ì •ê·œí™”ëœ í‘œì¤€ ì°½ê³ ëª…
    """
    if not warehouse_name or pd.isna(warehouse_name):
        return 'Unknown'
    
    warehouse_name = str(warehouse_name).strip()
    
    # ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € ì‹œë„
    for standard_name, variants in WAREHOUSE_NAME_MAPPING.items():
        if warehouse_name in variants:
            return standard_name
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    warehouse_name_lower = warehouse_name.lower()
    for standard_name, variants in WAREHOUSE_NAME_MAPPING.items():
        for variant in variants:
            if warehouse_name_lower in variant.lower() or variant.lower() in warehouse_name_lower:
                return standard_name
    
    # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    return warehouse_name

def to_num(s): 
    return pd.to_numeric(s, errors="coerce")

def close2(a, b, tol=TOL): 
    return (a is not None) and (b is not None) and abs(a - b) <= tol

# Ontology-based Utility Functions
def normalize_hvdc_code(code: str) -> str:
    """
    HVDC ì½”ë“œ ì •ê·œí™” (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    
    Args:
        code: ì •ê·œí™”í•  ì½”ë“œ ë¬¸ìì—´
    Returns:
        str: ì •ê·œí™”ëœ ì½”ë“œ
    """
    if not code or not isinstance(code, str):
        return ""
    
    # ê³µë°± ì œê±° ë° ëŒ€ë¬¸ìë¡œ ë³€í™˜
    normalized = str(code).strip().upper()
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•˜ì´í”ˆ ì œì™¸)
    normalized = re.sub(r'[^\w\-]', '', normalized)
    
    # ì—°ì†ëœ í•˜ì´í”ˆì„ í•˜ë‚˜ë¡œ í†µí•©
    normalized = re.sub(r'-+', '-', normalized)
    
    return normalized

def normalize_code_num(code: str) -> str:
    """HVDC CODE ìˆ«ì ë¶€ë¶„ ì •ê·œí™” (ì˜ˆ: 0014, 014, 14 â†’ 14)"""
    if not isinstance(code, str): 
        code = str(code)
    m = re.search(r'(\d+)$', code)
    return str(int(m.group(1))) if m else code

def codes_match(code1: str, code2: str, threshold: float = 0.9) -> bool:
    """
    HVDC ì½”ë“œ ë§¤ì¹­ - ìœ ì‚¬ë„ ê¸°ë°˜ (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    
    Args:
        code1, code2: ë¹„êµí•  ì½”ë“œë“¤
        threshold: ë§¤ì¹­ ì„ê³„ê°’
    Returns:
        bool: ë§¤ì¹­ ì—¬ë¶€
    """
    if not code1 or not code2:
        return False
    
    # ì½”ë“œ ì •ê·œí™”
    norm_code1 = normalize_hvdc_code(code1)
    norm_code2 = normalize_hvdc_code(code2)
    
    # ì™„ì „ ì¼ì¹˜ í™•ì¸
    if norm_code1 == norm_code2:
        return True
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    similarity = SequenceMatcher(None, norm_code1, norm_code2).ratio()
    return similarity >= threshold

def is_valid_hvdc_vendor(vendor_code: str, extended_mode: bool = False) -> bool:
    """
    HVDC ë²¤ë” ì½”ë“œ ìœ íš¨ì„± ê²€ì¦ (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    
    Args:
        vendor_code: ê²€ì¦í•  ë²¤ë” ì½”ë“œ
        extended_mode: í™•ì¥ ë²¤ë” ëª©ë¡ ì‚¬ìš© ì—¬ë¶€
    Returns:
        bool: ìœ íš¨ì„± ì—¬ë¶€
    """
    if not vendor_code or not isinstance(vendor_code, str):
        return False
        
    vendor_upper = str(vendor_code).strip().upper()
    
    if extended_mode:
        return vendor_upper in VENDOR_EXTENDED
    else:
        return vendor_upper in VENDOR_ALLOWED

def classify_warehouse_type(location: str) -> str:
    """
    ì°½ê³  ìœ„ì¹˜ ë¶„ë¥˜ (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    
    Args:
        location: ì°½ê³  ìœ„ì¹˜ëª…
    Returns:
        str: ì°½ê³  íƒ€ì… (Indoor/Outdoor/Site/Dangerous/Unknown)
    """
    if not location:
        return "Unknown"
        
    location_upper = str(location).strip().upper()
    
    if any(wh.upper() in location_upper for wh in WAREHOUSE_INDOOR):
        return "Indoor"
    elif any(wh.upper() in location_upper for wh in WAREHOUSE_OUTDOOR):
        return "Outdoor"
    elif any(wh.upper() in location_upper for wh in WAREHOUSE_SITE):
        return "Site"
    elif any(wh.upper() in location_upper for wh in WAREHOUSE_DANGEROUS):
        return "Dangerous"
    else:
        return "Unknown"

def get_billing_mode(warehouse: str) -> str:
    """
    âœ… NEW: ì°½ê³ ë³„ ê³¼ê¸ˆ ëª¨ë“œ ë¶„ë¥˜ í•¨ìˆ˜
    
    Args:
        warehouse: ì°½ê³ ëª…
    Returns:
        str: ê³¼ê¸ˆ ëª¨ë“œ ('rate'/'passthrough'/'no-charge'/'unknown')
    """
    if not warehouse:
        return "unknown"
        
    warehouse_clean = str(warehouse).strip()
    
    if warehouse_clean in BILLING_MODE_RATE:
        return "rate"
    elif warehouse_clean in BILLING_MODE_PASSTHROUGH:
        return "passthrough"
    elif warehouse_clean in BILLING_MODE_NO_CHARGE:
        return "no-charge"
    else:
        return "unknown"

def get_warehouse_rate(warehouse: str) -> float:
    """
    âœ… NEW: ì°½ê³ ë³„ ê³„ì•½ ë‹¨ê°€ ì¡°íšŒ í•¨ìˆ˜
    
    Args:
        warehouse: ì°½ê³ ëª…  
    Returns:
        float: ê³„ì•½ ë‹¨ê°€ (AED/sqm/month), ì—†ìœ¼ë©´ 0.0
    """
    return WAREHOUSE_RATES.get(warehouse, 0.0)

def load_invoice_passthrough_amounts(invoice_path: str) -> dict:
    """
    âœ… NEW: ì¸ë³´ì´ìŠ¤ì—ì„œ Passthrough ì°½ê³ ì˜ ì›”ë³„ ì´ì•¡ì„ ë¡œë“œ (ì‹¤ì œ ì¸ë³´ì´ìŠ¤ ì»¬ëŸ¼ ì‚¬ìš©)
    
    Args:
        invoice_path: ì¸ë³´ì´ìŠ¤ Excel íŒŒì¼ ê²½ë¡œ
    Returns:
        dict: {(YYYY-MM, Warehouse): total_amount} í˜•íƒœ
    """
    try:
        # ì¸ë³´ì´ìŠ¤ íŒŒì¼ ë¡œë“œ
        df = pd.read_excel(invoice_path, sheet_name=0)
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (ì‹¤ì œ ì¸ë³´ì´ìŠ¤ êµ¬ì¡°ì— ë§ê²Œ)
        df = df.rename(columns={
            'Amount_AED': 'Invoice_Amount',
            'Amount (AED)': 'Invoice_Amount', 
            'Total_Amount': 'Invoice_Amount',
            'Operation Date': 'Month',
            'Date': 'Month',
            'Invoice_Date': 'Month'
        })
        
        # ì›” ì»¬ëŸ¼ì„ YYYY-MM í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
        if 'Month' in df.columns:
            df['Month'] = pd.to_datetime(df['Month'], errors='coerce').dt.to_period('M').astype(str)
        else:
            print("âš ï¸ ì›” ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
            df['Month'] = '2024-01'  # ê¸°ë³¸ê°’
        
        # ì°½ê³  ì»¬ëŸ¼ í™•ì¸ ë° ì •ê·œí™”
        warehouse_col = None
        for col in ['Warehouse', 'Location', 'Storage_Location', 'WH']:
            if col in df.columns:
                warehouse_col = col
                break
        
        if warehouse_col is None:
            print("âš ï¸ ì°½ê³  ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
            df['Warehouse'] = 'Unknown'
            warehouse_col = 'Warehouse'
        
        # Invoice_Amount ì»¬ëŸ¼ í™•ì¸
        if 'Invoice_Amount' not in df.columns:
            print("âš ï¸ Invoice_Amount ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
            df['Invoice_Amount'] = 0.0
        
        # ì›”Ã—ì°½ê³ ë³„ ì´ì•¡ ì§‘ê³„
        passthrough_dict = {}
        grp = df.groupby(['Month', warehouse_col], dropna=False)['Invoice_Amount'].sum().reset_index()
        
        # dict í˜•íƒœë¡œ ë³€í™˜: {(YYYY-MM, Warehouse): amount}
        for _, row in grp.iterrows():
            month = row['Month']
            warehouse = row[warehouse_col]
            amount = float(row['Invoice_Amount'])
            
            if pd.notna(month) and pd.notna(warehouse) and amount > 0:
                passthrough_dict[(month, warehouse)] = amount
        
        print(f"âœ… Passthrough ê¸ˆì•¡ ë¡œë”© ì™„ë£Œ: {len(passthrough_dict)}ê°œ í•­ëª©")
        return passthrough_dict
        
    except Exception as e:
        print(f"âš ï¸ ì¸ë³´ì´ìŠ¤ passthrough ê¸ˆì•¡ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}

def split_hvdc_code(code: str):
    """
    Enhanced HVDC CODE ë¶„í•´ í•¨ìˆ˜ (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    
    Format: HVDC-ADOPT-HE-0325-1 â†’ ["HVDC","ADOPT","HE","0325","1"]
    - Part 1: HVDC (í”„ë¡œì íŠ¸ ì‹ë³„ì)
    - Part 2: ADOPT (í”„ë¡œì íŠ¸ íƒ€ì…)
    - Part 3: HE/SIM/SCT ë“± (ë²¤ë” ì½”ë“œ)
    - Part 4: 0325 (ìˆ«ì ID, 4ìë¦¬ íŒ¨ë”©)
    - Part 5: 1 (ì„œë¸Œ ì‹ë³„ì, ì˜µì…˜)
    """
    if not isinstance(code, str):
        return [None]*5
    
    # ì½”ë“œ ì •ê·œí™” ì ìš©
    normalized_code = normalize_hvdc_code(code)
    parts = [p.strip() for p in normalized_code.split("-") if p.strip()]
    
    # 5ê°œ íŒŒíŠ¸ë¡œ ë§ì¶¤ (ë¶€ì¡±í•œ ê²½ìš° Noneìœ¼ë¡œ ì±„ì›€)
    while len(parts) < 5:
        parts.append(None)
    
    return parts[:5]

def extract_parts(df, col_full="HVDC CODE", p1="HVDC CODE 1", p2="HVDC CODE 2", p3="HVDC CODE 3", p4="HVDC CODE 4", p5="HVDC CODE 5"):
    """
    Enhanced HVDC CODE íŒŒíŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ê¸°ë°˜)
    ì •ê·œí™” ë° ìœ íš¨ì„± ê²€ì¦ í¬í•¨
    """
    for c in [p1,p2,p3,p4,p5]:
        if c not in df.columns:
            df[c] = None
    
    def fill_row(row):
        parts = split_hvdc_code(row.get(col_full))
        for i,(cn,val) in enumerate(zip([p1,p2,p3,p4,p5], parts), start=1):
            if pd.isna(row.get(cn)) or row.get(cn) is None:
                row[cn] = val
        return row
    
    df = df.apply(fill_row, axis=1)
    
    # ì •ê·œí™” ë° ìœ íš¨ì„± ê²€ì¦
    for c in [p1,p2,p3,p4,p5]:
        df[c] = df[c].astype(str).str.strip().str.upper().replace({"NAN": None, "NONE": None})
    
    # ë²¤ë” ì½”ë“œ (Part 3) ìœ íš¨ì„± ê²€ì¦ - í™•ì¥ ëª¨ë“œ ì‚¬ìš©
    if p3 in df.columns:
        df[f"{p3}_VALID"] = df[p3].apply(lambda x: is_valid_hvdc_vendor(x, extended_mode=True))
    
    # ì°½ê³  ìœ„ì¹˜ ë¶„ë¥˜ ì¶”ê°€ (í•„ìš”ì‹œ)
    if 'Location' in df.columns:
        df['WAREHOUSE_TYPE'] = df['Location'].apply(classify_warehouse_type)
    
    return df

def expand_combined_codes(code: str):
    """
    Expand combined shorthand in a single string.
    Returns a set of full codes.
    """
    if not isinstance(code, str) or "," not in code:
        return {code} if isinstance(code, str) else set()
    code = code.replace(" ", "")
    # Split by comma
    parts = code.split(",")
    base = parts[0]  # full first token
    expanded = {base}
    # Base prefix up to last '-' before number-ish segment
    # Find the numeric block (with optional sub-suffix like -1)
    m = re.match(r"^(.*-)(\d+)(-[A-Za-z0-9]+)?$", base)
    if not m:
        # Try a simpler: up to last '-' then remainder
        prefix = base.rsplit("-", 1)[0] + "-"
        num_suffix = base.rsplit("-", 1)[-1]
        num_m = re.match(r"^(\d+)(-[A-Za-z0-9]+)?$", num_suffix)
        if num_m:
            num = num_m.group(1)
            sub = num_m.group(2) or ""
        else:
            num = None
            sub = ""
    else:
        prefix, num, sub = m.group(1), m.group(2), (m.group(3) or "")
    # Pad helper
    def pad4(x):
        return f"{int(x):04d}"
    for token in parts[1:]:
        # token could be like '0325-2' or '90' or '0090' or '195'
        t = token
        # If token includes '-', treat entire numeric-sub pattern
        if "-" in t:
            # assume full replacement numeric-sub
            num_part, sub_part = t.split("-", 1)
            if num_part.isdigit():
                full = f"{prefix}{pad4(num_part)}-{sub_part}"
                expanded.add(full)
            else:
                # fallback: just add as-is if looks full
                expanded.add(t)
        else:
            # purely digits => replace number, keep sub from base (if any? usually none)
            if t.isdigit() and num is not None:
                full = f"{prefix}{pad4(t)}{sub or ''}"
                expanded.add(full)
            else:
                expanded.add(t)
    return expanded

def explode_by_pkg(df_subset):
    """
    íŒ¨í‚¤ì§€ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ explodeí•˜ì—¬ ê° íŒ¨í‚¤ì§€ë³„ ë‹¨ìœ„ ë°ì´í„° ìƒì„±
    (ONTOLOGY ê¸°ë°˜ ê°œì„ )
    
    Args:
        df_subset: DataFrame with Pkg, G.W(kgs), CBM columns
        
    Returns:
        DataFrame: exploded data with unit weights/volumes per package
    """
    if df_subset.empty:
        return pd.DataFrame(columns=["Pkg", "G.W(kgs)", "CBM"])
    
    exploded_rows = []
    
    for idx, row in df_subset.iterrows():
        pkg_count = int(row.get("Pkg", 0))
        if pkg_count <= 0:
            continue
            
        unit_gw = float(row.get("G.W(kgs)", 0)) / pkg_count
        unit_cbm = float(row.get("CBM", 0)) / pkg_count
        
        # ê° íŒ¨í‚¤ì§€ë³„ë¡œ ë‹¨ìœ„ ë°ì´í„° ìƒì„±
        for pkg_idx in range(pkg_count):
            exploded_rows.append({
                "Original_Index": idx,
                "Pkg_Unit": pkg_idx + 1,
                "Pkg": 1,  # ê° ë‹¨ìœ„ëŠ” 1 íŒ¨í‚¤ì§€
                "G.W(kgs)": unit_gw,
                "CBM": unit_cbm
            })
    
    return pd.DataFrame(exploded_rows)

def exact_subset_match(pkgs_df, k, gw_tgt, cbm_tgt, tol=TOL):
    idxs = list(pkgs_df.index)
    arr_gw  = pkgs_df["G.W(kgs)"].values
    arr_cbm = pkgs_df["CBM"].values
    for comb in combinations(range(len(idxs)), k):
        gw = float(np.sum(arr_gw[list(comb)]))
        cbm = float(np.sum(arr_cbm[list(comb)]))
        if close2(gw, gw_tgt, tol) and close2(cbm, cbm_tgt, tol):
            picked = [idxs[i] for i in comb]
            return True, picked, gw, cbm
    return False, [], None, None

def greedy_init(pkgs_df, k, gw_tgt, cbm_tgt):
    w = pkgs_df["G.W(kgs)"].values
    c = pkgs_df["CBM"].values
    ratio_target = gw_tgt / max(cbm_tgt, 1e-6)
    ratio = w / np.clip(c, 1e-6, None)
    score2 = np.abs(ratio - ratio_target)
    gw_norm = (w / max(gw_tgt, 1e-6))
    cbm_norm = (c / max(cbm_tgt, 1e-6))
    score = np.abs(gw_norm - gw_norm.mean()) + np.abs(cbm_norm - cbm_norm.mean())
    s = 0.6*score2 + 0.4*score
    picked = list(pkgs_df.assign(_s=s).sort_values("_s").index[:k])
    gw_sum = float(pkgs_df.loc[picked, "G.W(kgs)"].sum())
    cbm_sum = float(pkgs_df.loc[picked, "CBM"].sum())
    return picked, gw_sum, cbm_sum

def local_swap_improve(pkgs_df, picked, gw_tgt, cbm_tgt, tol=TOL, max_iter=400):
    picked = list(picked)
    all_idx = set(pkgs_df.index)
    cur_gw  = float(pkgs_df.loc[picked, "G.W(kgs)"].sum())
    cur_cbm = float(pkgs_df.loc[picked, "CBM"].sum())
    def err(gw, cbm): return abs(gw - gw_tgt) + abs(cbm - cbm_tgt)
    best_err = err(cur_gw, cur_cbm)
    for _ in range(max_iter):
        improved = False
        for out_i in list(picked):
            if out_i not in picked: 
                continue
            for in_i in list(all_idx - set(picked)):
                new_gw  = cur_gw  - pkgs_df.at[out_i, "G.W(kgs)"] + pkgs_df.at[in_i, "G.W(kgs)"]
                new_cbm = cur_cbm - pkgs_df.at[out_i, "CBM"]      + pkgs_df.at[in_i, "CBM"]
                new_err = err(new_gw, new_cbm)
                if new_err < best_err:
                    new_picked = picked.copy()
                    if out_i in new_picked:
                        new_picked.remove(out_i)
                        new_picked.append(in_i)
                        picked = new_picked
                        cur_gw, cur_cbm, best_err = new_gw, new_cbm, new_err
                        improved = True
                        if close2(cur_gw, gw_tgt, tol) and close2(cur_cbm, cbm_tgt, tol):
                            return picked, cur_gw, cur_cbm
            if improved: break
        if not improved: break
    return picked, cur_gw, cur_cbm

def robust_greedy_local(values_gw, values_cbm, k, gw_tgt, cbm_tgt, tol=TOL, max_iter=300):
    """
    Enhanced robust greedy local search (ONTOLOGY ê¸°ë°˜ ê°œì„ )
    ê¸°ì¡´ greedy_local í•¨ìˆ˜ì˜ mutation ë¬¸ì œë¥¼ í•´ê²°í•œ robust ë²„ì „
    
    Args:
        values_gw, values_cbm: numpy arrays of weights/volumes
        k: number of items to select
        gw_tgt, cbm_tgt: target weights/volumes
        tol: tolerance for matching
        max_iter: maximum iterations for local search
        
    Returns:
        tuple: (success, picked_indices, sum_gw, sum_cbm)
    """
    n = len(values_gw)
    if n < k or k <= 0:
        return False, [], None, None
        
    # Enhanced scoring system with ontology-based weights
    ratio_target = gw_tgt / max(cbm_tgt, 1e-6)
    ratio = values_gw / np.clip(values_cbm, 1e-6, None)
    score2 = np.abs(ratio - ratio_target)
    
    # Normalized scores for balance
    gw_norm = (values_gw / max(gw_tgt, 1e-6))
    cbm_norm = (values_cbm / max(cbm_tgt, 1e-6))
    score = np.abs(gw_norm - gw_norm.mean()) + np.abs(cbm_norm - cbm_norm.mean())
    
    # Combined scoring with enhanced weights
    combined_score = 0.6 * score2 + 0.4 * score
    
    # Initial greedy selection - immutable approach
    picked_indices = list(np.argsort(combined_score)[:k])
    gw = float(values_gw[picked_indices].sum())
    cbm = float(values_cbm[picked_indices].sum())
    
    # Early return if already optimal
    if close2(gw, gw_tgt, tol) and close2(cbm, cbm_tgt, tol):
        return True, picked_indices, gw, cbm

    def calculate_error(indices):
        """Calculate total error for given indices"""
        if len(indices) == 0:
            return float('inf')
        gw_sum = values_gw[indices].sum()
        cbm_sum = values_cbm[indices].sum()
        return abs(gw_sum - gw_tgt) + abs(cbm_sum - cbm_tgt)

    # Robust local search with immutable operations
    best_indices = picked_indices.copy()
    best_error = calculate_error(np.array(best_indices))
    
    for iteration in range(max_iter):
        improved = False
        current_indices = best_indices.copy()
        
        # Try swapping each selected item
        for i in range(k):
            current_set = set(current_indices)
            out_idx = current_indices[i]
            
            best_local_error = best_error
            best_replacement = None
            
            # Try all possible replacements
            for in_idx in range(n):
                if in_idx in current_set:
                    continue
                    
                # Create trial indices without mutation
                trial_indices = current_indices.copy()
                trial_indices[i] = in_idx
                
                trial_error = calculate_error(np.array(trial_indices))
                
                if trial_error < best_local_error:
                    best_local_error = trial_error
                    best_replacement = in_idx
                    
                    # Early termination if perfect match found
                    if trial_error == 0:
                        break
            
            # Apply best improvement if found
            if best_replacement is not None and best_local_error < best_error:
                current_indices[i] = best_replacement
                best_error = best_local_error
                improved = True
                
                # Check if target reached
                gw_sum = float(values_gw[current_indices].sum())
                cbm_sum = float(values_cbm[current_indices].sum())
                if close2(gw_sum, gw_tgt, tol) and close2(cbm_sum, cbm_tgt, tol):
                    return True, current_indices, gw_sum, cbm_sum
        
        # Update best solution if improved
        if improved:
            best_indices = current_indices.copy()
        else:
            break  # No improvement found, terminate
    
    # Final calculation
    final_gw = float(values_gw[best_indices].sum())
    final_cbm = float(values_cbm[best_indices].sum())
    success = close2(final_gw, gw_tgt, tol) and close2(final_cbm, cbm_tgt, tol)
    
    return success, best_indices, final_gw, final_cbm

def find_subset_match(pkgs_df, k, gw_tgt, cbm_tgt, tol=TOL):
    """
    Enhanced subset matching with robust algorithms (ONTOLOGY ê¸°ë°˜)
    """
    N = len(pkgs_df)
    if N < k or k <= 0:
        return {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "invalid"}
    
    # For small datasets, use exact matching
    if N <= MAX_EXACT_N:
        ok, picked, gw, cbm = exact_subset_match(pkgs_df, k, gw_tgt, cbm_tgt, tol)
        return {"found": ok, "picked": picked, "sum_gw": gw, "sum_cbm": cbm, "method": "exact"}
    
    # For large datasets, use robust greedy-local approach
    values_gw = pkgs_df["G.W(kgs)"].values.astype(float)
    values_cbm = pkgs_df["CBM"].values.astype(float)
    
    success, picked_indices, sum_gw, sum_cbm = robust_greedy_local(
        values_gw, values_cbm, k, gw_tgt, cbm_tgt, tol
    )
    
    # Convert indices back to DataFrame indices
    picked_df_indices = [pkgs_df.index[i] for i in picked_indices] if picked_indices else []
    
    return {
        "found": success,
        "picked": picked_df_indices,
        "sum_gw": sum_gw,
        "sum_cbm": sum_cbm,
        "method": "robust-greedy-local"
    }

def find_subset_match_exploded(cand_df, k, gw_tgt, cbm_tgt, tol=TOL):
    """
    Exploded íŒ¨í‚¤ì§€ ë‹¨ìœ„ ë§¤ì¹­ (ONTOLOGY ê¸°ë°˜ ê°œì„ )
    ê° íŒ¨í‚¤ì§€ë¥¼ ê°œë³„ ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ì—¬ ë” ì •í™•í•œ ë§¤ì¹­ ìˆ˜í–‰
    
    Args:
        cand_df: candidate DataFrame
        k: target package count
        gw_tgt, cbm_tgt: target weights/volumes
        tol: tolerance
        
    Returns:
        dict: matching result
    """
    if cand_df.empty or k <= 0:
        return {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "no-candidate-exploded"}
    
    # Explode by package units
    units = explode_by_pkg(cand_df[["Pkg", "G.W(kgs)", "CBM"]])
    
    if len(units) == 0:
        return {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "no-units-exploded"}
    
    # Extract values for matching
    vals_gw = units["G.W(kgs)"].values.astype(float)
    vals_cbm = units["CBM"].values.astype(float)
    
    # Choose matching strategy based on size
    if len(units) <= MAX_EXACT_N:
        # Exact matching for small datasets
        arr_gw = vals_gw
        arr_cbm = vals_cbm
        found_exact = False
        picked_indices = []
        sum_gw = sum_cbm = None
        
        for comb in combinations(range(len(arr_gw)), k):
            gw = float(np.sum(arr_gw[list(comb)]))
            cbm = float(np.sum(arr_cbm[list(comb)]))
            if close2(gw, gw_tgt, tol) and close2(cbm, cbm_tgt, tol):
                picked_indices = list(comb)
                sum_gw, sum_cbm = gw, cbm
                found_exact = True
                break
        
        return {
            "found": found_exact,
            "picked": picked_indices,
            "sum_gw": sum_gw,
            "sum_cbm": sum_cbm,
            "method": "exact-exploded"
        }
    else:
        # Use robust greedy-local for large datasets
        success, picked_indices, sum_gw, sum_cbm = robust_greedy_local(
            vals_gw, vals_cbm, k, gw_tgt, cbm_tgt, tol
        )
        
        return {
            "found": success,
            "picked": picked_indices,
            "sum_gw": sum_gw,
            "sum_cbm": sum_cbm,
            "method": "robust-greedy-local-exploded"
        }

def enhanced_subset_matching(cand_df, k, gw_tgt, cbm_tgt, tol=TOL, use_exploded=True):
    """
    Enhanced subset matching with multiple strategies (ONTOLOGY ê¸°ë°˜)
    
    Args:
        cand_df: candidate DataFrame
        k: target package count
        gw_tgt, cbm_tgt: target weights/volumes
        tol: tolerance
        use_exploded: whether to use exploded matching
        
    Returns:
        dict: best matching result
    """
    if cand_df.empty or k <= 0:
        return {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "no-candidate"}
    
    results = []
    
    # Strategy 1: Original DataFrame matching
    result1 = find_subset_match(cand_df[["G.W(kgs)", "CBM"]], k, gw_tgt, cbm_tgt, tol)
    if result1["found"]:
        results.append(result1)
    
    # Strategy 2: Exploded matching (if enabled)
    if use_exploded and "Pkg" in cand_df.columns:
        result2 = find_subset_match_exploded(cand_df, k, gw_tgt, cbm_tgt, tol)
        if result2["found"]:
            results.append(result2)
    
    # Return best result (prioritize exact matches, then by accuracy)
    if not results:
        return result1  # Return original result even if not found
    
    # Prioritize exact methods, then by accuracy
    exact_results = [r for r in results if "exact" in r["method"]]
    if exact_results:
        return exact_results[0]
    
    # Among non-exact results, pick the one with smallest error
    def calculate_total_error(result):
        if (not result["found"] or 
            result["sum_gw"] is None or 
            result["sum_cbm"] is None or
            len(result.get("picked", [])) == 0):
            return float('inf')
        gw_error = abs(result["sum_gw"] - gw_tgt)
        cbm_error = abs(result["sum_cbm"] - cbm_tgt)
        return gw_error + cbm_error
    
    best_result = min(results, key=calculate_total_error)
    return best_result

def row_key(ix, row):
    return f"{ix}|GW={row['G.W(kgs)']:.2f}|CBM={row['CBM']:.2f}"

# Load
df_inv = pd.read_excel(INVOICE_PATH, sheet_name='Invoice_Original')
df_all = pd.read_excel(ALL_PATH, sheet_name=0)

# Numeric
for col in ["No. of Pkgs", "Weight (kg)", "CBM"]:
    if col in df_inv.columns: df_inv[col] = to_num(df_inv[col])
for col in ["Pkg", "G.W(kgs)", "CBM"]:
    if col in df_all.columns: df_all[col] = to_num(df_all[col])

# Dates
if ALL_DATE_COL in df_all.columns:
    df_all["_ym"] = pd.to_datetime(df_all[ALL_DATE_COL], errors="coerce").dt.to_period("M").astype(str)
else:
    df_all["_ym"] = None
if INV_DATE_COL in df_inv.columns:
    df_inv["_ym"] = pd.to_datetime(df_inv[INV_DATE_COL], errors="coerce").dt.to_period("M").astype(str)
else:
    df_inv["_ym"] = None

# Parts fill
df_inv = extract_parts(df_inv, col_full="HVDC CODE")
df_all = extract_parts(df_all, col_full="HVDC CODE")

match_rows = []
detail_rows = []

for raw_code in df_inv["HVDC CODE"].dropna().unique():
    inv_rows = df_inv[df_inv["HVDC CODE"] == raw_code]
    # Expand combined codes from raw_code
    expanded = expand_combined_codes(raw_code)
    
    # Extract REV NO information for identification
    rev_nos = inv_rows["REV NO"].dropna().unique() if "REV NO" in inv_rows.columns else []
    rev_no_list = ", ".join([str(rev) for rev in sorted(rev_nos)]) if len(rev_nos) > 0 else "N/A"
    rev_no_count = len(rev_nos) if len(rev_nos) > 0 else 0
    
    # Enhanced vendor code analysis (from invoice parts)
    p1, p2, p3, p4 = inv_rows.iloc[0]["HVDC CODE 1"], inv_rows.iloc[0]["HVDC CODE 2"], inv_rows.iloc[0]["HVDC CODE 3"], inv_rows.iloc[0]["HVDC CODE 4"]
    vendor = str(p3).upper() if p3 else None
    
    # Enhanced vendor validation with ontology system
    is_primary_vendor = is_valid_hvdc_vendor(vendor, extended_mode=False)  # HE, SIM only
    is_extended_vendor = is_valid_hvdc_vendor(vendor, extended_mode=True)  # All known vendors
    
    if is_primary_vendor:
        vmemo = "PRIMARY_VENDOR"
    elif is_extended_vendor:
        vmemo = "EXTENDED_VENDOR"
    else:
        vmemo = "NO_DATA"
        
    ym = inv_rows["_ym"].dropna().unique()
    ym = ym[0] if len(ym)>0 else None

    # Build candidate pool as union of:
    #  - FULL code in expanded set
    #  - OR parts(1..4) exactly equal to invoice parts (for each expanded code we treat same parts base)
    cand = df_all[
        (df_all["HVDC CODE"].isin(expanded)) |
        ((df_all["HVDC CODE 1"] == p1) & (df_all["HVDC CODE 2"] == p2) & (df_all["HVDC CODE 3"] == p3) & (df_all["HVDC CODE 4"] == p4))
    ].copy()

    # Enhanced filtering logic - allow extended vendors but prefer primary vendors
    if not is_extended_vendor:
        cand = cand.iloc[0:0]  # empty if vendor not recognized at all

    if USE_MONTH_FILTER and ym is not None and cand["_ym"].notna().any():
        cand = cand[cand["_ym"] == ym]

    # Targets
    k = int(inv_rows["No. of Pkgs"].sum(skipna=True))
    gw_tgt = float(inv_rows["Weight (kg)"].sum(skipna=True))
    cbm_tgt = float(inv_rows["CBM"].sum(skipna=True))

    # ğŸ”§ PATCH: Pkg PASS íŒì •ì„ sum(Pkg) ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½
    N = len(cand)
    all_pkgs_sum = int(cand["Pkg"].fillna(0).sum()) if len(cand) > 0 else 0
    pkg_pass = (all_pkgs_sum >= k)

    # ğŸ”§ PATCH: í›„ë³´ ì—†ìŒ/íŒ¨í‚¤ì§€ ë¶€ì¡±ì‹œ ì¡°ê¸° ì¢…ë£Œ
    if len(cand) == 0 or all_pkgs_sum == 0 or k <= 0:
        result = {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "no-candidate"}
        err_gw = err_cbm = None
        gw_ok = cbm_ok = False
        match_status = "FAIL"
    else:
        # ğŸ”§ PATCH: í•­ìƒ unit ë‹¨ìœ„ë¡œ ë³€í™˜ (Pkg ìˆ˜ë§Œí¼ ë¶„í•´, GW/CBMì€ Pkgë¡œ ê· ë“±ë¶„ë°°)
        units = explode_by_pkg(cand[["Pkg", "G.W(kgs)", "CBM"]])
        
        if len(units) == 0:
            result = {"found": False, "picked": [], "sum_gw": None, "sum_cbm": None, "method": "no-units"}
            err_gw = err_cbm = None
            gw_ok = cbm_ok = False
            match_status = "FAIL"
        else:
            vals_gw = units["G.W(kgs)"].values.astype(float)
            vals_cbm = units["CBM"].values.astype(float)
            
            if len(units) <= MAX_EXACT_N:
                # ğŸ”§ PATCH: ì •í™• ë§¤ì¹­ (exploded ê¸°ì¤€)
                found_exact = False
                picked_indices = []
                sum_gw = sum_cbm = None
                
                for comb in combinations(range(len(vals_gw)), k):
                    gw = float(np.sum(vals_gw[list(comb)]))
                    cbm = float(np.sum(vals_cbm[list(comb)]))
                    if close2(gw, gw_tgt, TOL) and close2(cbm, cbm_tgt, TOL):
                        picked_indices = list(comb)
                        sum_gw, sum_cbm = gw, cbm
                        found_exact = True
                        break
                
                result = {
                    "found": found_exact,
                    "picked": picked_indices,
                    "sum_gw": sum_gw,
                    "sum_cbm": sum_cbm,
                    "method": "exact-exploded"
                }
            else:
                # ğŸ”§ PATCH: Robust ê·¸ë¦¬ë””-ë¡œì»¬ ë§¤ì¹­ (exploded ê¸°ì¤€)
                success, picked_indices, sum_gw, sum_cbm = robust_greedy_local(
                    vals_gw, vals_cbm, k, gw_tgt, cbm_tgt, TOL
                )
                
                result = {
                    "found": success,
                    "picked": picked_indices,
                    "sum_gw": sum_gw,
                    "sum_cbm": sum_cbm,
                    "method": "robust-greedy-local-exploded"
                }
            
            # ğŸ”§ PATCH: ì—ëŸ¬ ë° ë§¤ì¹˜ ìƒíƒœ ê³„ì‚°
            err_gw = None if result["sum_gw"] is None else (result["sum_gw"] - gw_tgt)
            err_cbm = None if result["sum_cbm"] is None else (result["sum_cbm"] - cbm_tgt)
            gw_ok = (result["sum_gw"] is not None and abs(err_gw) <= TOL)
            cbm_ok = (result["sum_cbm"] is not None and abs(err_cbm) <= TOL)
            match_status = "PASS" if (pkg_pass and gw_ok and cbm_ok) else "FAIL"

    # ğŸ”§ PATCH: Picked keys ì²˜ë¦¬ (exploded unit ê¸°ì¤€)
    picked_keys = []
    if result["picked"] and result["method"] not in ["no-candidate", "no-units"]:
        # exploded unitì—ì„œ ì„ íƒëœ ì¸ë±ìŠ¤ë“¤ ì²˜ë¦¬
        if "exploded" in result["method"]:
            # units DataFrameì—ì„œ ì •ë³´ ì¶”ì¶œ
            if 'units' in locals() and len(units) > 0:
                for unit_idx in result["picked"]:
                    if unit_idx < len(units):
                        unit_row = units.iloc[unit_idx]
                        original_idx = unit_row.get("Original_Index", unit_idx)
                        unit_gw = unit_row["G.W(kgs)"]
                        unit_cbm = unit_row["CBM"]
                        picked_keys.append(f"Unit_{unit_idx}|GW={unit_gw:.3f}|CBM={unit_cbm:.3f}")
                        
                        # ì›ë³¸ í–‰ ì •ë³´ë„ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                        if original_idx in cand.index:
                            orig_row = cand.loc[original_idx]
                            detail_rows.append({
                                "REV_NO_List": rev_no_list,
                                "REV_NO_Count": rev_no_count,
                                "Invoice_RAW_CODE": raw_code,
                                "Expanded_Code_Member?": orig_row["HVDC CODE"] in expanded,
                                "HVDC CODE": orig_row["HVDC CODE"],
                                "Picked_Unit_Idx": unit_idx,
                                "Original_Row_Idx": original_idx,
                                "Unit_GW": unit_gw,
                                "Unit_CBM": unit_cbm,
                                "Original_Total_GW": orig_row["G.W(kgs)"],
                                "Original_Total_CBM": orig_row["CBM"],
                                "Original_Pkg_Count": orig_row.get("Pkg", 1),
                                "_ym": orig_row.get("_ym", None),
                                "Vendor(code3)": vendor,
                                "Vendor_Type": vmemo,
                                "Warehouse_Type": classify_warehouse_type(orig_row.get("Location", "")),
                                "Code_Normalized": normalize_hvdc_code(orig_row["HVDC CODE"])
                            })
        else:
            # ê¸°ì¡´ ë°©ì‹ (ë¹„-exploded)
            for ix in result["picked"]:
                if ix in cand.index:
                    row = cand.loc[ix]
                    picked_keys.append(row_key(ix, row))
                    detail_rows.append({
                        "REV_NO_List": rev_no_list,
                        "REV_NO_Count": rev_no_count,
                        "Invoice_RAW_CODE": raw_code,
                        "Expanded_Code_Member?": row["HVDC CODE"] in expanded,
                        "HVDC CODE": row["HVDC CODE"],
                        "Picked_Idx": ix,
                        "GW": row["G.W(kgs)"],
                        "CBM": row["CBM"],
                        "_ym": row.get("_ym", None),
                        "Vendor(code3)": vendor,
                        "Vendor_Type": vmemo,
                        "Warehouse_Type": classify_warehouse_type(row.get("Location", "")),
                        "Code_Normalized": normalize_hvdc_code(row["HVDC CODE"])
                    })

    # Enhanced result analysis
    method_used = result.get("method", "unknown")
    is_exploded = "exploded" in method_used
    is_exact = "exact" in method_used
    is_robust = "robust" in method_used
    
    # ğŸ”§ PATCH: ê¶Œì¥ ì¶œë ¥ ì»¬ëŸ¼ìœ¼ë¡œ ê²°ê³¼ ì €ì¥ (ë¦¬í¬íŠ¸ ê°€ë…ì„±â†‘)
    match_rows.append({
        # ğŸ¯ ì‹ë³„ ì •ë³´
        "REV_NO_List": rev_no_list,
        "REV_NO_Count": rev_no_count,
        "Invoice_RAW_CODE": raw_code,
        "Expanded_Set": ", ".join(sorted(expanded)),
        
        # ğŸ¯ íŒ¨í‚¤ì§€ ë¶„ì„ (í•µì‹¬!)
        "Invoice_Pkgs(k)": k,
        "All_Pkgs(sum)": all_pkgs_sum,  # ğŸ”§ PATCH: sum(Pkg) ê¸°ì¤€ ì¶”ê°€
        "Candidate_Rows(N)": N,
        "Pkg_Status": "PASS" if pkg_pass else "FAIL",  # ğŸ”§ PATCH: sum(Pkg) ê¸°ì¤€ íŒì •
        
        # ğŸ¯ ë¬´ê²Œ/ë¶€í”¼ ë¶„ì„
        "GW_Invoice": gw_tgt,
        "CBM_Invoice": cbm_tgt,
        "GW_SumPicked": result.get("sum_gw"),
        "CBM_SumPicked": result.get("sum_cbm"),
        "Err_GW": err_gw,  # ğŸ”§ PATCH: ì§ì ‘ ê³„ì‚°ëœ ì—ëŸ¬ê°’ ì‚¬ìš©
        "Err_CBM": err_cbm,  # ğŸ”§ PATCH: ì§ì ‘ ê³„ì‚°ëœ ì—ëŸ¬ê°’ ì‚¬ìš©
        
        # ğŸ¯ ë§¤ì¹˜ ê²°ê³¼ (í•µì‹¬!)
        "GW_Match(Â±0.10)": "PASS" if gw_ok else "FAIL",  # ğŸ”§ PATCH: ê°œì„ ëœ íŒì •
        "CBM_Match(Â±0.10)": "PASS" if cbm_ok else "FAIL",  # ğŸ”§ PATCH: ê°œì„ ëœ íŒì •
        "Match_Status": match_status,  # ğŸ”§ PATCH: ì¢…í•© ë§¤ì¹˜ ìƒíƒœ ì¶”ê°€
        
        # ğŸ¯ ì•Œê³ ë¦¬ì¦˜ ì •ë³´
        "Method": method_used,
        "Algorithm_Quality": "EXACT" if is_exact else ("ROBUST" if is_robust else "BASIC"),
        "Is_Exploded_Method": is_exploded,
        "Is_Exact_Method": is_exact,
        "Is_Robust_Method": is_robust,
        
        # ğŸ¯ ë²¤ë” ì •ë³´
        "Vendor(code3)": vendor,
        "Vendor_Type": vmemo,
        "Is_Primary_Vendor": is_primary_vendor,
        "Is_Extended_Vendor": is_extended_vendor,
        
        # ğŸ¯ ê³¼ê¸ˆ ëª¨ë“œ ì •ë³´ (NEW) - ì°½ê³  ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì • (ì •ê·œí™” ì ìš©)
        "Billing_Mode": get_billing_mode(normalize_warehouse_name(inv_rows.iloc[0].get("Location", "") if "Location" in inv_rows.columns else "")),
        "Contract_Rate_AED": get_warehouse_rate(normalize_warehouse_name(inv_rows.iloc[0].get("Location", "") if "Location" in inv_rows.columns else "")),
        
        # ğŸ¯ ìƒì„¸ ì •ë³´
        "Picked_Count": len(result["picked"]) if result.get("picked") else 0,
        "Picked_List": "; ".join(picked_keys),
        "Code_Normalized": normalize_hvdc_code(raw_code),
        "Data_Scope": vmemo  # Keep for backward compatibility
    })

# ğŸ”§ NEW: ì‚¬ìš©ì-ì¹œí™”í˜• ì¸ë³´ì´ìŠ¤ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
df_match = pd.DataFrame(match_rows)
df_detail = pd.DataFrame(detail_rows)

# ğŸ¯ 1) Exceptions_Only ì‹œíŠ¸ ìƒì„± (ì˜ˆì™¸ ì „ìš©)
def create_exceptions_only():
    """FAIL ìƒíƒœë§Œ í¬í•¨í•œ ì˜ˆì™¸ ì „ìš© ì‹œíŠ¸ ìƒì„±"""
    # Operation Date ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
    if "Operation Date" not in df_match.columns:
        # df_invì—ì„œ HVDC CODE ë§¤ì¹­ìœ¼ë¡œ Operation Date ê°€ì ¸ì˜¤ê¸°
        date_mapping = {}
        for _, row in df_inv.iterrows():
            hvdc_code = row.get("HVDC CODE")
            op_date = row.get("Operation Date")
            if hvdc_code and op_date:
                date_mapping[hvdc_code] = op_date
        
        df_match["Operation Date"] = df_match["Invoice_RAW_CODE"].map(date_mapping)
    
    # ì˜ˆì™¸ ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§ (PASSê°€ ì•„ë‹Œ ëª¨ë“  ê²½ìš°)
    df_exceptions = df_match.loc[df_match["Match_Status"] != "PASS"].copy()
    
    # ì—ëŸ¬ í•©ê³„ë¡œ ì •ë ¬ (ê°€ì¥ ì‹¬ê°í•œ ì—ëŸ¬ë¶€í„°)
    df_exceptions["Total_Error"] = (
        df_exceptions["Err_GW"].fillna(0).abs() + 
        df_exceptions["Err_CBM"].fillna(0).abs()
    )
    
    # ì •ë ¬: FAIL â†’ ì—ëŸ¬ í•©ê³„ ë‚´ë¦¼ì°¨ìˆœ â†’ Operation Date
    sort_columns = ["Match_Status", "Total_Error"]
    sort_ascending = [True, False]
    
    if "Operation Date" in df_exceptions.columns:
        sort_columns.append("Operation Date")
        sort_ascending.append(True)
        
    df_exceptions = df_exceptions.sort_values(by=sort_columns, ascending=sort_ascending)
    
    # ì»¬ëŸ¼ ì„ íƒ ë° ìˆœì„œ ì •ë¦¬ (ì •ë ¬ í›„)
    exception_cols = [
        "Operation Date", "Invoice_RAW_CODE", "Vendor(code3)",
        "Invoice_Pkgs(k)", "All_Pkgs(sum)", "Pkg_Status",
        "GW_Invoice", "GW_SumPicked", "Err_GW",
        "CBM_Invoice", "CBM_SumPicked", "Err_CBM",
        "GW_Match(Â±0.10)", "CBM_Match(Â±0.10)", "Match_Status",
        "Method", "Picked_Count", "REV_NO_List"
    ]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_cols = [col for col in exception_cols if col in df_exceptions.columns]
    df_ex = df_exceptions[available_cols].copy()
    
    return df_ex

# ğŸ¯ 2) ì›ë³¸ ìˆœì„œ ì‹œíŠ¸ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ê°œì„ )
def create_invoice_original_order_sheet():
    """ì›ë³¸ ì¸ë³´ì´ìŠ¤ ìˆœì„œ + ë§¤ì¹­ ê²°ê³¼ë¥¼ ê²°í•©í•œ ì‹œíŠ¸ ìƒì„±"""
    df_invoice_original = df_inv.copy()
    
    # ë§¤ì¹­ ê²°ê³¼ ë§¤í•‘
    match_dict = {}
    for _, row in df_match.iterrows():
        hvdc_code = row['Invoice_RAW_CODE']
        match_dict[hvdc_code] = {
            'All_Pkgs(sum)': row.get('All_Pkgs(sum)', 0),
            'Candidate_Units': row.get('All_Pkgs(sum)', 0),  # ë³´ì¡° ì •ë³´
            'Pkg_Status': row.get('Pkg_Status', 'NO_MATCH'),
            'GW_SumPicked': row.get('GW_SumPicked', None),
            'CBM_SumPicked': row.get('CBM_SumPicked', None),
            'Err_GW': row.get('Err_GW', None),
            'Err_CBM': row.get('Err_CBM', None),
            'GW_Match': row.get('GW_Match(Â±0.10)', 'NO_MATCH'),
            'CBM_Match': row.get('CBM_Match(Â±0.10)', 'NO_MATCH'),
            'Match_Status': row.get('Match_Status', 'NO_MATCH'),
            'Method': row.get('Method', 'NO_MATCH'),
            'Picked_Count': row.get('Picked_Count', 0)
        }
    
    # ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
    result_columns = []
    for _, row in df_invoice_original.iterrows():
        hvdc_code = row.get('HVDC CODE', '')
        if hvdc_code in match_dict:
            result_columns.append(match_dict[hvdc_code])
        else:
            result_columns.append({
                'All_Pkgs(sum)': 0,
                'Candidate_Units': 0,
                'Pkg_Status': 'NO_MATCH',
                'GW_SumPicked': None,
                'CBM_SumPicked': None,
                'Err_GW': None,
                'Err_CBM': None,
                'GW_Match': 'NO_MATCH',
                'CBM_Match': 'NO_MATCH',
                'Match_Status': 'NO_MATCH',
                'Method': 'NO_MATCH',
                'Picked_Count': 0
            })
    
    # ë°ì´í„° ê²°í•©
    df_results = pd.DataFrame(result_columns)
    df_combined = pd.concat([df_invoice_original, df_results], axis=1)
    
    # REV NO ìˆœì„œëŒ€ë¡œ ì •ë ¬
    if 'REV NO' in df_combined.columns:
        df_combined = df_combined.sort_values(['REV NO'], na_position='last')
    
    return df_combined

# ğŸ¯ 3) Dashboard KPI ê³„ì‚°
def calculate_dashboard_kpi():
    """ëŒ€ì‹œë³´ë“œìš© KPI ì§€í‘œ ê³„ì‚°"""
    total_lines = len(df_match)
    fail_count = (df_match["Match_Status"] != "PASS").sum()
    pass_count = (df_match["Match_Status"] == "PASS").sum()
    fail_ratio = round(100 * (fail_count / total_lines), 1) if total_lines > 0 else 0
    
    # ì—ëŸ¬ í‰ê·  (ì ˆëŒ“ê°’ ê¸°ì¤€)
    avg_err_gw = round(df_match["Err_GW"].fillna(0).abs().mean(), 3)
    avg_err_cbm = round(df_match["Err_CBM"].fillna(0).abs().mean(), 3)
    
    # ë©”ì„œë“œë³„ ì„±ëŠ¥
    method_stats = df_match.groupby("Method").agg({
        "Match_Status": lambda x: (x == "PASS").sum(),
        "Invoice_RAW_CODE": "count"
    }).rename(columns={"Invoice_RAW_CODE": "Total"})
    
    method_stats["Success_Rate"] = round(100 * method_stats["Match_Status"] / method_stats["Total"], 1)
    
    # Top ì˜ˆì™¸ ì¼€ì´ìŠ¤ (ìƒìœ„ 10ê±´)
    df_top_exceptions = df_match.loc[df_match["Match_Status"] != "PASS"].copy()
    if not df_top_exceptions.empty:
        df_top_exceptions["Total_Error"] = (
            df_top_exceptions["Err_GW"].fillna(0).abs() + 
            df_top_exceptions["Err_CBM"].fillna(0).abs()
        )
        df_top_exceptions = df_top_exceptions.nlargest(10, "Total_Error")[
            ["Invoice_RAW_CODE", "Invoice_Pkgs(k)", "All_Pkgs(sum)", 
             "Err_GW", "Err_CBM", "Match_Status", "Method"]
        ]
    
    return {
        "KPI": {
            "ì´_ë¼ì¸ìˆ˜": total_lines,
            "PASS": pass_count,
            "FAIL": fail_count,
            "FAIL_ë¹„ìœ¨(%)": fail_ratio,
            "í‰ê· _Err_GW": avg_err_gw,
            "í‰ê· _Err_CBM": avg_err_cbm
        },
        "Method_Performance": method_stats,
        "Top_Exceptions": df_top_exceptions
    }

# ë°ì´í„° ìƒì„±
df_exceptions = create_exceptions_only()
df_invoice_order = create_invoice_original_order_sheet()
dashboard_data = calculate_dashboard_kpi()

# ğŸ¯ 4) ì‚¬ìš©ì-ì¹œí™”í˜• Excel ì¶œë ¥
with pd.ExcelWriter(OUT_PATH, engine="xlsxwriter") as writer:
    workbook = writer.book
    
    # === ìŠ¤íƒ€ì¼ ì •ì˜ ===
    header_format = workbook.add_format({
        'bold': True, 'text_wrap': True, 'valign': 'top',
        'fg_color': '#D7E4BD', 'border': 1
    })
    
    info_format = workbook.add_format({
        'fg_color': '#E6F3FF', 'border': 1
    })
    
    pass_format = workbook.add_format({
        'font_color': '#006400', 'bold': True  # ì´ˆë¡ìƒ‰
    })
    
    fail_format = workbook.add_format({
        'font_color': '#9C0006', 'bold': True  # ë¹¨ê°•ìƒ‰
    })
    
    warn_format = workbook.add_format({
        'bg_color': '#FFF2CC', 'border': 1  # ë…¸ë‘ìƒ‰ ë°°ê²½
    })
    
    error_format = workbook.add_format({
        'bg_color': '#FFC7CE', 'border': 1  # ë¹¨ê°•ìƒ‰ ë°°ê²½
    })
    
    number_format = workbook.add_format({'num_format': '#,##0.00'})
    integer_format = workbook.add_format({'num_format': '#,##0'})
    
    # === 1) Dashboard ì‹œíŠ¸ ===
    # KPI í…Œì´ë¸”
    kpi_df = pd.DataFrame([dashboard_data["KPI"]])
    kpi_df.to_excel(writer, index=False, sheet_name="Dashboard", startrow=1)
    
    # ë©”ì„œë“œ ì„±ëŠ¥ í…Œì´ë¸”
    dashboard_data["Method_Performance"].to_excel(
        writer, sheet_name="Dashboard", startrow=4, startcol=0
    )
    
    # Top ì˜ˆì™¸ ì¼€ì´ìŠ¤
    if not dashboard_data["Top_Exceptions"].empty:
        dashboard_data["Top_Exceptions"].to_excel(
            writer, index=False, sheet_name="Dashboard", startrow=4, startcol=6
        )
    
    # Dashboard ì œëª© ì¶”ê°€
    dashboard_ws = writer.sheets["Dashboard"]
    dashboard_ws.write(0, 0, "ğŸ“Š HVDC ì¸ë³´ì´ìŠ¤ ê²€ì¦ ëŒ€ì‹œë³´ë“œ", workbook.add_format({'bold': True, 'font_size': 16}))
    dashboard_ws.write(3, 0, "ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥", workbook.add_format({'bold': True, 'font_size': 12}))
    dashboard_ws.write(3, 6, "Top ì˜ˆì™¸ ì¼€ì´ìŠ¤ (ìƒìœ„ 10ê±´)", workbook.add_format({'bold': True, 'font_size': 12}))
    
    # === 2) Exceptions_Only ì‹œíŠ¸ ===
    df_exceptions.to_excel(writer, index=False, sheet_name="Exceptions_Only")
    exceptions_ws = writer.sheets["Exceptions_Only"]
    
    # í—¤ë” í¬ë§· ì ìš©
    for col_num, value in enumerate(df_exceptions.columns.values):
        exceptions_ws.write(0, col_num, value, header_format)
    
    # Freeze Panes (1í–‰ + 5ì—´ ê³ ì •)
    exceptions_ws.freeze_panes(1, 5)
    
    # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
    exceptions_ws.set_column(0, 4, 12)
    exceptions_ws.set_column(5, 20, 14)
    
    # ì¡°ê±´ë¶€ ì„œì‹ ì ìš©
    if len(df_exceptions) > 0:
        # Match_Status FAIL ê°•ì¡°
        match_col = df_exceptions.columns.get_loc("Match_Status") if "Match_Status" in df_exceptions.columns else -1
        if match_col >= 0:
            exceptions_ws.conditional_format(
                1, match_col, len(df_exceptions), match_col,
                {'type': 'text', 'criteria': 'containing', 'value': 'FAIL', 'format': fail_format}
            )
        
        # Pkg_Status FAIL ê°•ì¡°
        pkg_col = df_exceptions.columns.get_loc("Pkg_Status") if "Pkg_Status" in df_exceptions.columns else -1
        if pkg_col >= 0:
            exceptions_ws.conditional_format(
                1, pkg_col, len(df_exceptions), pkg_col,
                {'type': 'text', 'criteria': 'containing', 'value': 'FAIL', 'format': warn_format}
            )
    
    # === 3) Invoice_Original_Order ì‹œíŠ¸ ===
    df_invoice_order.to_excel(writer, index=False, sheet_name="Invoice_Original_Order")
    invoice_ws = writer.sheets["Invoice_Original_Order"]
    
    # í—¤ë” í¬ë§·
    for col_num, value in enumerate(df_invoice_order.columns.values):
        invoice_ws.write(0, col_num, value, header_format)
    
    # Freeze Panes
    invoice_ws.freeze_panes(1, 5)
    
    # ì»¬ëŸ¼ ë„ˆë¹„ ë° ì„œì‹
    invoice_ws.set_column(0, 4, 12)
    invoice_ws.set_column(5, len(df_inv.columns)-1, 14)
    
    # ê²°ê³¼ ë¸”ë¡ ë°°ê²½ìƒ‰ (í•˜ëŠ˜ìƒ‰)
    result_start_col = len(df_inv.columns)
    for col in range(result_start_col, len(df_invoice_order.columns)):
        invoice_ws.set_column(col, col, 14, info_format)
    
    # === 4) Picked_Detail ì‹œíŠ¸ ===
    if not df_detail.empty:
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        detail_cols = ["Invoice_RAW_CODE", "Original_Row_Idx", "Picked_Unit_Idx", 
                      "Unit_GW", "Unit_CBM", "Original_Total_GW", "Original_Total_CBM", 
                      "Original_Pkg_Count", "Vendor(code3)", "Warehouse_Type"]
        available_detail_cols = [col for col in detail_cols if col in df_detail.columns]
        df_detail_ordered = df_detail[available_detail_cols + [col for col in df_detail.columns if col not in available_detail_cols]]
        
        df_detail_ordered.to_excel(writer, index=False, sheet_name="Picked_Detail")
        detail_ws = writer.sheets["Picked_Detail"]
        
        # í—¤ë” í¬ë§·
        for col_num, value in enumerate(df_detail_ordered.columns.values):
            detail_ws.write(0, col_num, value, header_format)
        
        # Freeze Panes
        detail_ws.freeze_panes(1, 3)
        detail_ws.set_column(0, 10, 14)

print(f"Saved: {OUT_PATH}")
print("ğŸ¯ ì‚¬ìš©ì-ì¹œí™”í˜• ì¸ë³´ì´ìŠ¤ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
print("ğŸ“‹ ìƒì„±ëœ ì‹œíŠ¸ (ì‚¬ìš© ìˆœì„œ):")
print("  1. ğŸ“Š Dashboard - KPI ìš”ì•½ ë° í•„í„°")
print("  2. âš ï¸  Exceptions_Only - ì˜ˆì™¸ ì¼€ì´ìŠ¤ ì „ìš© (FAILë§Œ)")
print("  3. ğŸ“ Invoice_Original_Order - ì›ë³¸ ìˆœì„œ + ê²°ê³¼")
print("  4. ğŸ” Picked_Detail - ë§¤ì¹­ ê·¼ê±° ìƒì„¸")
print("\nğŸ’¡ ì‚¬ìš©ë²•:")
print("  â€¢ Dashboardì—ì„œ ì „ì²´ í˜„í™© íŒŒì•…")
print("  â€¢ Exceptions_Onlyì—ì„œ ë¬¸ì œ ì¼€ì´ìŠ¤ ìš°ì„  ê²€í† ")  
print("  â€¢ Invoice_Original_Orderì—ì„œ ì›ë³¸ ëŒ€ì¡°")
print("  â€¢ Picked_Detailì—ì„œ ë§¤ì¹­ ê·¼ê±° í™•ì¸")

# âœ… NEW: ê³¼ê¸ˆ ëª¨ë“œ í†µí•© ë° Passthrough ê¸ˆì•¡ ì—°ê²° ì˜ˆì‹œ
print("\nğŸ¢ ê³¼ê¸ˆ ëª¨ë“œ í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰ (v3.0-corrected ì—°ë™)")

try:
    # 1) Passthrough ê¸ˆì•¡ ë¡œë“œ
    passthrough_amounts = load_invoice_passthrough_amounts(INVOICE_PATH)
    print(f"ğŸ“Š Passthrough ê¸ˆì•¡ ë¡œë“œ: {len(passthrough_amounts)}ê°œ í•­ëª©")
    
    # 2) hvdc_excel_reporter_final_sqm_rev.pyì™€ ì—°ë™í•˜ì—¬ ê³¼ê¸ˆ ê³„ì‚°
    try:
        # hvdc_excel_reporter_final_sqm_rev.py ì„í¬íŠ¸
        import sys
        sys.path.append('.')
        from hvdc_excel_reporter_final_sqm_rev import HVDCExcelReporterFinal
        
        # Reporter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        reporter = HVDCExcelReporterFinal()
        
        # 1) ì¸ë³´ì´ìŠ¤ ë¡œë“œ (ìŠ¤í‚¤ë§ˆ: Operation Date, TOTAL)
        invoice_df = pd.read_excel(INVOICE_PATH, sheet_name=0)
        # ì»¬ëŸ¼ëª…ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        invoice_df = invoice_df.rename(columns={
            'Operation Date': 'Month',
            'TOTAL': 'Invoice_Amount'
        })
        # Warehouse ì»¬ëŸ¼ ì¶”ê°€ (ê¸°ë³¸ê°’ìœ¼ë¡œ 'Unknown' ì„¤ì •)
        invoice_df['Warehouse'] = 'Unknown'
        
        # 2) Passthrough dict êµ¬ì„±
        passthrough = reporter.calculator.build_passthrough_amounts(invoice_df)
        print(f"ğŸ“Š Passthrough dict ìƒì„±: {len(passthrough)}ê°œ í•­ëª©")
        
        # 3) ì‹œìŠ¤í…œ í†µê³„ ì‚°ì¶œ(ê¸°ì¡´)
    stats = reporter.calculate_warehouse_statistics()
        
        # 4) ì¼í• +ëª¨ë“œ ê³¼ê¸ˆìœ¼ë¡œ êµì²´
    stats['sqm_invoice_charges'] = reporter.calculator.calculate_monthly_invoice_charges_prorated(
            stats['processed_data'],
            passthrough_amounts=passthrough
        )
        
        # 5) ê³¼ê¸ˆ ì‹œíŠ¸ ìƒì„±
        invoice_sheet_df = reporter.create_sqm_invoice_sheet(stats)
        print(f"âœ… SQM Invoice ê³¼ê¸ˆ ì‹œíŠ¸ ìƒì„± ì™„ë£Œ: {len(invoice_sheet_df)}ê±´")
        
        # 6) ğŸ¯ NEW: Monthly_Charges_Match ì‹œíŠ¸ ìƒì„±
        print("ğŸ“Š Monthly_Charges_Match ì‹œíŠ¸ ìƒì„± ì¤‘...")
        
        # Monthly_Charges_Match ì‹œíŠ¸ ìƒì„± í•¨ìˆ˜ (ì¸ë¼ì¸)
        def create_monthly_charges_match_inline(reporter, stats: dict, invoice_df: pd.DataFrame, delta_thr=0.02) -> pd.DataFrame:
            """ì›”Ã—ì°½ê³  ë‹¨ìœ„ System vs Invoice ë§¤ì¹­"""
            # 1) System í…Œì´ë¸” ì „ê°œ
            charges = stats.get('sqm_invoice_charges', {})
            sys_rows = []
            for ym, payload in charges.items():
                for wh, v in payload.items():
                    if wh == 'total_monthly_charge_aed' or not isinstance(v, dict):
                        continue
                    sys_rows.append({
                        'Month': ym,
                        'Warehouse': wh,
                        'Billing_Mode': v.get('billing_mode', ''),
                        'System_Avg_SQM': v.get('avg_sqm', 0.0),
                        'System_Rate': v.get('rate_aed', 0.0),
                        'System_Amount': v.get('monthly_charge_aed', 0.0),
                        'Amount_Source': v.get('amount_source', '')
                    })
        df_sys = pd.DataFrame(sys_rows)

        # 2) Invoice í…Œì´ë¸” ì •ê·œí™”
        inv = invoice_df.copy()
        inv['Month'] = pd.to_datetime(inv['Month'], errors='coerce').dt.to_period('M').astype(str)
        inv = inv.rename(columns={
            'Amount_AED': 'Invoice_Amount',
            'Rate_AED_per_SQM': 'Invoice_Rate',
            'Billed_SQM': 'Invoice_SQM'
        })
        for col in ['Invoice_Amount','Invoice_Rate','Invoice_SQM']:
            if col not in inv.columns: inv[col] = np.nan
        inv_grp = inv.groupby(['Month','Warehouse'], dropna=False)[['Invoice_Amount','Invoice_Rate','Invoice_SQM']].sum().reset_index()

        # 3) í‚¤ ì¡°ì¸
        m = df_sys.merge(inv_grp, on=['Month','Warehouse'], how='left', validate='one_to_one')

        # 4) ê³„ì•½ ë‹¨ê°€/ëª¨ë“œ ê¸°ëŒ€ì¹˜
        get_mode = reporter.calculator.billing_mode.get
        get_rate = reporter.calculator.warehouse_sqm_rates.get
        m['Expected_Mode'] = m['Warehouse'].map(get_mode).fillna('unknown')
        m['Contract_Rate'] = m['Warehouse'].map(get_rate).fillna(0.0)

        # 5) System ê¸ˆì•¡ ì¬ì‚°ì¶œ(ê²€ì¦ìš©)
        def _recalc_system_amount(row):
            mode = row['Expected_Mode']
            if mode == 'rate':
                return round(float(row['System_Avg_SQM']) * float(row['Contract_Rate']), 2)
            elif mode == 'passthrough':
                return float(row['System_Amount'])
            else:  # no-charge
                return 0.0
        m['System_Amount_Recalc'] = m.apply(_recalc_system_amount, axis=1)

        # 6) Î” ê³„ì‚°
        m['Invoice_Amount'] = m['Invoice_Amount'].fillna(0.0).astype(float)
        m['Î”_AED'] = m['Invoice_Amount'] - m['System_Amount_Recalc']
        m['Î”_%'] = np.where(m['Invoice_Amount'] == 0, 0.0, m['Î”_AED'] / m['Invoice_Amount'])

        # 7) PASS/FAIL
        def _status(row):
            mode = row['Expected_Mode']
            inv_amt = float(row['Invoice_Amount'])
            sys_amt = float(row['System_Amount_Recalc'])
            if mode == 'rate':
                return "PASS" if abs(row['Î”_%']) <= delta_thr else "FAIL"
            elif mode == 'passthrough':
                return "PASS" if abs(inv_amt - sys_amt) < 0.5 else "FAIL"
            else:  # no-charge
                return "PASS" if inv_amt == 0.0 else "FAIL"
        m['Status'] = m.apply(_status, axis=1)

        # 8) Reason_Code
        def _reason(row):
            mode = row['Expected_Mode']
            if mode == 'unknown':
                return REASON["MODE_MISSING"]
            if mode == 'rate':
                if not np.isnan(row.get('Invoice_Rate', np.nan)) and abs(row['Invoice_Rate'] - row['Contract_Rate']) > 1e-6:
                    return REASON["RATE_DIFF"]
                if row['Status'] == 'FAIL':
                    return REASON["PRORATION_MISMATCH"]
                return ""
            if mode == 'passthrough':
                if row['Status'] == 'FAIL':
                    return REASON["PASSTHROUGH_MISMATCH"]
                return ""
            if mode == 'no-charge':
                if row['Invoice_Amount'] > 0:
                    return REASON["NOCHARGE_VIOLATION"]
                return ""
            return ""
        m['Reason_Code'] = m.apply(_reason, axis=1)

        # 9) ì •ë¦¬ ì»¬ëŸ¼
        out_cols = [
            'Month','Warehouse','Expected_Mode','Billing_Mode',
            'System_Avg_SQM','Contract_Rate','System_Amount_Recalc',
            'Invoice_SQM','Invoice_Rate','Invoice_Amount',
            'Î”_AED','Î”_%','Status','Reason_Code','Amount_Source'
        ]
        return m[out_cols].sort_values(['Month','Warehouse']).reset_index(drop=True)
        
        match_df = create_monthly_charges_match_inline(reporter, stats, invoice_df, delta_thr=0.02)
        print(f"âœ… Monthly_Charges_Match ì‹œíŠ¸ ì™„ë£Œ: {len(match_df)}ê±´")
        
        # 7) ğŸ¯ NEW: Exceptions_and_Evidence ì‹œíŠ¸ ìƒì„±
        print("âš ï¸ Exceptions_and_Evidence ì‹œíŠ¸ ìƒì„± ì¤‘...")
        
        # Exceptions_and_Evidence ì‹œíŠ¸ ìƒì„± í•¨ìˆ˜ (ì¸ë¼ì¸)
        def create_exceptions_and_evidence_inline(match_df: pd.DataFrame, stats: dict, delta_thr=0.02) -> pd.DataFrame:
            """FAIL/WARNë§Œ ì¶”ì¶œ. ì¦ë¹™ ê²½ë¡œ ì»¬ëŸ¼ì„ êµ¬ì„±"""
            if match_df is None or match_df.empty:
                return pd.DataFrame(columns=[
                    'Month','Warehouse','Expected_Mode','Contract_Rate',
                    'System_Avg_SQM','System_Amount_Recalc','Invoice_Amount',
                    'Î”_AED','Î”_%','Status','Reason_Code',
                    'Evidence_Flow_Timeline','Evidence_Daily_Occupancy','Evidence_Source_Sheet'
                ])

            df = match_df.copy()
            # ë“±ê¸‰í™”
            def _grade(row):
                if row['Expected_Mode'] == 'rate':
                    if abs(row['Î”_%']) <= delta_thr: return 'PASS'
                    if abs(row['Î”_%']) <= 0.05: return 'WARN'
                    return 'FAIL'
                return row['Status']
            df['Grade'] = df.apply(_grade, axis=1)

            ex = df[df['Grade'] != 'PASS'].copy()

            # ì¦ë¹™ ë§í¬
            ex['Evidence_Flow_Timeline']   = 'Flow_Timeline'
            ex['Evidence_Daily_Occupancy'] = 'SQM_í”¼ë²—í…Œì´ë¸”'
            ex['Evidence_Source_Sheet']    = 'ì›ë³¸_ë°ì´í„°_ìƒ˜í”Œ'

            keep = [
                'Month','Warehouse','Expected_Mode','Contract_Rate',
                'System_Avg_SQM','System_Amount_Recalc','Invoice_Amount',
                'Î”_AED','Î”_%','Grade','Status','Reason_Code',
                'Evidence_Flow_Timeline','Evidence_Daily_Occupancy','Evidence_Source_Sheet'
            ]
            return ex[keep].sort_values(['Month','Warehouse']).reset_index(drop=True)
        
        exceptions_df = create_exceptions_and_evidence_inline(match_df, stats, delta_thr=0.02)
        print(f"âœ… Exceptions_and_Evidence ì‹œíŠ¸ ì™„ë£Œ: {len(exceptions_df)}ê±´")
        
        # 8) í†µí•© Excel íŒŒì¼ ì €ì¥
        output_path = "HVDC_Invoice_Validation_Dashboard_with_Billing.xlsx"
        with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
            # ê¸°ì¡´ ê³¼ê¸ˆ ì‹œíŠ¸
            invoice_sheet_df.to_excel(writer, sheet_name="SQM_Invoiceê³¼ê¸ˆ", index=False)
            
            # ğŸ¯ NEW: ë§¤ì¹­ ì‹œíŠ¸
            match_df.to_excel(writer, sheet_name="Monthly_Charges_Match", index=False)
            
            # ğŸ¯ NEW: ì˜ˆì™¸ ì‹œíŠ¸
            exceptions_df.to_excel(writer, sheet_name="Exceptions_and_Evidence", index=False)
        
        print(f"ğŸ’¾ í†µí•© ê³¼ê¸ˆ ì‹œíŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   - SQM_Invoiceê³¼ê¸ˆ: {len(invoice_sheet_df)}ê±´")
        print(f"   - Monthly_Charges_Match: {len(match_df)}ê±´")
        print(f"   - Exceptions_and_Evidence: {len(exceptions_df)}ê±´")
        
    except Exception as e:
        print(f"âš ï¸ Reporter ì—°ë™ ì‹¤íŒ¨: {e}")
        print("   (hvdc_excel_reporter_final_sqm_rev.py íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)")
    
    # 3) ê³¼ê¸ˆ ëª¨ë“œë³„ ìš”ì•½ í†µê³„
    billing_summary = {
        'rate_warehouses': list(BILLING_MODE_RATE),
        'passthrough_warehouses': list(BILLING_MODE_PASSTHROUGH), 
        'no_charge_warehouses': list(BILLING_MODE_NO_CHARGE),
        'total_passthrough_amount': sum(passthrough_amounts.values()),
        'rate_based_count': len(BILLING_MODE_RATE),
        'passthrough_count': len(BILLING_MODE_PASSTHROUGH),
        'no_charge_count': len(BILLING_MODE_NO_CHARGE)
    }
    
    print(f"âœ… ê³¼ê¸ˆ ëª¨ë“œ ìš”ì•½:")
    print(f"   - Rate ê¸°ë°˜: {billing_summary['rate_based_count']}ê°œ ì°½ê³  (ê³„ì•½ë‹¨ê°€ ì ìš©)")
    print(f"   - Passthrough: {billing_summary['passthrough_count']}ê°œ ì°½ê³  (ì¸ë³´ì´ìŠ¤ ì´ì•¡)")  
    print(f"   - No-charge: {billing_summary['no_charge_count']}ê°œ ì°½ê³  (ê³¼ê¸ˆ ì—†ìŒ)")
    print(f"   - Passthrough ì´ì•¡: {billing_summary['total_passthrough_amount']:,.2f} AED")
    
except Exception as e:
    print(f"âš ï¸ ê³¼ê¸ˆ ëª¨ë“œ í†µí•© ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


